














# Loading the necessary libraries
import pandas as pd # To manipulate the dataset
import numpy as np # for any statistics/averages
import matplotlib.pyplot as plt # visiaulaisation library
import seaborn as sns # more extensive visualization library
%matplotlib inline


#load the datasets loaded
df1 = pd.read_csv('Data/AviationData.csv',encoding='ISO-8859-1')
df2 = pd.read_csv('Data/USState_Codes.csv',encoding='ISO-8859-1')


#This is to get a general overview of the datasets.
df1.head(5)


df2.head(5)


#Info is to understand the columns and their data types.
df1.info()


df2.info()


'''
This brief overview shows that they are 88889 rows of data in the Aviation dataset and they are 62 rows in,
inside the US city dataset.
Under head you have see a number of NaN values, I also note that,
all columns in the first data set are non-null while that of the second data set,
has many object data columns most likely inconsistent data types.
'''





df1.isna().sum() #The isna shows the true or false if null values are found then sum() adds the all.


'''
Multiple duplicates have been found in date, Total entries columns, and No of engines.
this will help me determine if the Nulls need to be filled or dropped.
They are only dropped if the impact will be small compared to the dataset.
'''


#FILL
#The number of missing rows is very large compared to actual entries of 88889.
# So for all integer/float columns tye Nulls will be filled with the mean value.
# This will hopefully present less loss of data and not affect the analysis.
Mean_Number_of_Engines = df1['Number.of.Engines'].mean()
Mean_Fatal_Injuries = df1['Total.Fatal.Injuries'].mean()
Mean_Serious_Injuries = df1['Total.Serious.Injuries'].mean()
Mean_Minor_Injuries = df1['Total.Minor.Injuries'].mean()
Mean_Uninjured = df1['Total.Uninjured'].mean()


df1['Number.of.Engines'] = df1['Number.of.Engines'].fillna(Mean_Number_of_Engines)
df1['Total.Fatal.Injuries'] = df1['Total.Fatal.Injuries'].fillna(Mean_Fatal_Injuries)
df1['Total.Serious.Injuries'] = df1['Total.Serious.Injuries'].fillna(Mean_Serious_Injuries)
df1['Total.Minor.Injuries'] = df1['Total.Minor.Injuries'].fillna(Mean_Minor_Injuries)
df1['Total.Uninjured'] = df1['Total.Uninjured'].fillna(Mean_Uninjured)


df1.isna().sum() 


#Drop
#For the smaller null columns like location, make, model, Amateur.Built, Country we will drop these.
#It is also difficult to find the average of categorical values, as they can all be unique and replacing them may affect the analysis.

#Drop smaller values
df1 = df1.dropna(subset= ['Location', 'Country', 'Make', 'Model','Amateur.Built', 'Injury.Severity'])
#Drop Missing dates as we want to do a Time series later on and weather conditions which I believe will be critical.
df1 = df1.dropna(subset= ['Publication.Date','Weather.Condition'])
#I could not drop the other columns as it would significantly reduce the data.


df1.isna().sum() #To check if they are still nulls.


#Because we still have some many missing values we can fill them with N/A
df1 = df1.fillna('N/A')


df1.isna().sum() 





#I remove duplicates in the data using the ID as I assume this is unique and should be single.
df1[df1['Event.Id'].duplicated()]


'''
They are 747 rows of duplicated data , I will drop these to ensure data entegrity.
'''


#to drop I use drop duplicates then specify the event column and I state I only wnat the first ID to stay.
df1 = df1.drop_duplicates(subset=['Event.Id'],keep= 'first')


df1[df1['Event.Id'].duplicated()] #Just to check that the duplicates are gone.





# to change the data types as it could cause errors when doing analysis.
#As the totals columns and Number.of.Engines are already float.
String_columns = [
'Latitude',
'Longitude',
'Location',
'Country',
'Airport.Code',
'Airport.Name',
'Injury.Severity',
'Aircraft.damage',
'Aircraft.Category',
'Registration.Number',
'Make',
'Model',
'Amateur.Built',
'Engine.Type',
'FAR.Description',
'Schedule',
'Purpose.of.flight',
'Air.carrier',
'Weather.Condition',
'Broad.phase.of.flight',
'Report.Status']

df1[String_columns] = df1[String_columns].apply(str)


df1.info()





'''
I want to start exploratory Data analysis however we still have the second dataset.
To explore both I need to join the data sets together so that,
they can both be analysied, also the other dataset is very small.
'''


#I am using inner join as I only want records that match in both datasets.
df = df1.join(df2, how = 'inner')



